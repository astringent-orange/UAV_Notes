
# Motivating
![[48d7774e-7e00-4057-952a-fbcfd9e2e483.png]]
回顾之前的内容 mean estimation，通过统计平均值去近似X的期望

如果求这个“统计平均值”？有两种方法
1. 通过真实采样，等所有数据收集完成，然后加在一起求平均，耗时长
2. 增量式方法，来几个计算几个
![[b8ce2642-5502-44b4-86ef-b6bf9dc54538.png]]
通过迭代的方法，每得到一个采样更新一次平均

![[7a9a27fd-561c-47e2-b345-896565cd7448.png]]
如果将迭代算法改成这样，是否还能收敛到期望值？当αk满足一定条件时是可以的。而这种算法也是SA和SGD的一种特殊形式。

# Robbins-monro algorithm
首先stochastic approximation表示了一大类随机且迭代的算法，且用于方程求解或进行优化问题，其特点在于不需要知道目标方程或表达式。而RM是SA开创的工作，而SGD是SA的一种特殊情况。

假设要求解以下这个方程，且不知道函数的表达式
![[ddcd63f0-a5d0-44fc-b7d7-237b40433822.png]]

RM算法如下
![[77bdcb5d-b994-43e9-bcdc-2130d18eeeeb.png]]
![[3dabef69-6d1f-489d-a7f3-823b7b8df096.png]]
不断通过迭代w，便最终可以得到g(w)=0的解。其中η表示噪音，而g(w, η)表示g(w)带噪音的观测，ak表示一个正因子；整个过程已知的只有输入的{w}和输出的带噪结果{g(w,η)}

当g(w)满足以下条件时RM算法是可以实现的
![[8ed025ab-45ed-4f92-9ec4-ad29dc65cd33.png]]
第一个条件g(w)的梯度要求为正且有界，即g(w)单调增；第二个条件如何解读：首先ak^2 和趋于无穷，表示ak最后一定会趋于0，而ak和趋于无穷要求不要收敛得太快；第三个条件，即η的均值为0，且方差有界。

更具体而言

# Stochastic gradient descent



# BGD MBGD SGD