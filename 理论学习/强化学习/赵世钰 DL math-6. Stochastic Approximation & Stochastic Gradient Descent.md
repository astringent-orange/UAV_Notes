
# Motivating
![[48d7774e-7e00-4057-952a-fbcfd9e2e483.png]]
回顾之前的内容 mean estimation，通过统计平均值去近似X的期望

如果求这个“统计平均值”？有两种方法
1. 通过真实采样，等所有数据收集完成，然后加在一起求平均，耗时长
2. 增量式方法，来几个计算几个
![[b8ce2642-5502-44b4-86ef-b6bf9dc54538.png]]
通过迭代的方法，每得到一个采样更新一次平均

![[7a9a27fd-561c-47e2-b345-896565cd7448.png]]
如果将迭代算法改成这样，是否还能收敛到期望值？当αk满足一定条件时是可以的。而这种算法也是SA和SGD的一种特殊形式。

# Robbins-monro algorithm
首先stochastic approximation表示了一大类随机且迭代的算法，且用于方程求解或进行优化问题，其特点在于不需要知道目标方程或表达式。而RM是SA开创的工作，而SGD是SA的一种特殊情况。

假设要求解以下这个方程，且不知道函数的表达式
![[ddcd63f0-a5d0-44fc-b7d7-237b40433822.png]]

RM算法如下
![[77bdcb5d-b994-43e9-bcdc-2130d18eeeeb.png]]
![[3dabef69-6d1f-489d-a7f3-823b7b8df096.png]]
不断通过迭代w，便最终可以得到g(w)=0的解。其中η表示噪音，而g(w, η)表示g(w)带噪音的观测，ak表示一个正因子；整个过程已知的只有输入的{w}和输出的带噪结果{g(w,η)}

当g(w)满足以下条件时RM算法是可以实现的
![[8ed025ab-45ed-4f92-9ec4-ad29dc65cd33.png]]
第一个条件g(w)的梯度要求为正且有界，即g(w)单调增；第二个条件如何解读：首先ak^2 和趋于无穷，表示ak最后一定会趋于0，而ak和趋于无穷要求不要收敛得太快；第三个条件，即η的均值为0，且方差有界。

更具体而言，对第二个条件进行讨论
首先，为什么说ak趋向于0是重要的呢？可以将上面的迭代公式写为以下形式，如果ak趋向于0，那么左边也趋向于0，表示wk+1和wk趋于相等
![[0f64b2c9-8a72-41a9-8805-4d66d4b22001.png]]
其次，为什么ak和等于无穷，即让ak不要太快到0，为什么这个式子重要？对于一开始的迭代公式逐项相加，有以下的形式。w1表示最初猜测的值，而w∞为最终的解w* ，当ak和是有界数时，这两者的差则也可能是一个有界的数，那么w1就不能随便，必须要求在w* 附近。
![[52625f82-3827-491b-9dce-23ea16663daf.png]]
最后，什么样的ak可以满足这两个条件呢？通常一个典型就是ak=1/k。但实际使用时，一般会让ak为一个很小的常数
![[0a0a4d20-5c52-44c5-8810-fbc3dc9c6c1c.png]]

回到一开始的mean estimation，当时说当ak满足一些条件时，wk仍然可以收敛到EX；这个算法其实可以看为RM
![[7a9a27fd-561c-47e2-b345-896565cd7448.png]]
考虑以下这样一个函数，目标是求g(w)=0，就相当于去求EX
![[bb2f049c-d918-4f67-bb1f-0c0d31611f57.png]]


![[63da20a9-b399-4997-8510-131aa2e53afc.png]]

# Stochastic gradient descent
SGD假设有以下这样的优化问题，其中w为可修改的参数，而X是一个分布确定的随机变量。而已知最小值处梯度为0，为了求这样的优化问题，逐步有以下三个方法
![[b11ee92f-8720-4141-ba80-0723772fa461.png]]
1. gradient descent GD 梯度下降
![[7e580472-6f24-4b49-8ce3-5a5d1c62a991.png]]
其中αk代表步长；注意求梯度的符号可以拿到求期望里面。但是该方法有一个问题，怎么求这个梯度的期望。一种是有模型f，另一种则是没有模型但有数据

2. batch gradient descent BGD 批量梯度下降
![[97fb32a2-0fcc-4cb4-b409-a76341044140.png]]
同样，想要求梯度的期望，于是就用多次采样的均值。但是实际中仍然有问题，即每次更新wk都要采样很多次数据，于是有第三种方法

3. stochastic gradient descent SGD 随机梯度下降
![[5525dd9e-d436-4960-8d01-9c20430f9202.png]]
每采样一次，就可以更新一次参数；用一个随机采样的梯度来近似真实（期望）梯度

如何证明该方法收敛？证明SGD是一种特殊的RM算法，而RM在满足一些条件时是收敛的即可；
找原函数的最小值，即找梯度函数为0的点->转换为RM问题，g(w)=0；而RM求解方法是，不知道g(w)的公式，但是有一些带噪样本，而随机梯度本身就可以视为带噪样本
![[1a62a70c-2dc0-422e-9390-f3a7a21997de.png]]
于是查看RM要求满足的条件：
(1)g(w)是单调增的，即f(w,x)是一个凸函数；(2)ak满足条件；(3)采样iid

下面来讨论一下SGD收敛过程中的一些表现
1. SGD的收敛速度：因为有随机采样，所以想要到达收敛需要很久呢
![[19a2b4d7-b7c4-4f77-92b5-711fed947689.png]]
定义一个相对误差，发现w离最优w* 较远时，SGD体现出来的效果和GD相似，只有在w* 附近时，可能体现出来一些随机性，证明如下
![[b94a7c0c-c5bc-44e3-9cb1-724943dc6a72.png]]
因为最优值处导数的期望为0，所以可以直接加在分母中；然后用拉格朗日中值定理有右式；而之前假设过f为凸函数，所以其二阶导大于0，从而有以下内容
![[2d5bad0e-7a7b-4120-9db4-79a7e4e79da4.png]]

# BGD MBGD SGD