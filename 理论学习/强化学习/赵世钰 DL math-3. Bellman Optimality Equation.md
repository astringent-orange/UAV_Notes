
# Motivating examples
当策略并不是太好时，该如何改进？->计算action value，选择最大的

# Definition
比较两个策略
![[9250871b-28b9-452e-91fd-074dac8ba0de.png]]
如果一个策略在所有状态下都比另一个策略好，则说明其“更好”；从而有“最优”的定义，即一个策略比所有策略都好
从而有许多问题：最优是否存在？是否唯一？是确定的还是非确定的？如何得到

**Bellman optimality equation BOE**
![[1c4dca59-a07f-44d5-aa2d-8ee94992d59d.png]]贝尔曼最优可以表示为 之前贝尔曼公式中 加上max；

有矩阵形式如下
![[43f41480-664d-43e5-b5e5-bd2363f7dc9a.png]]

贝尔曼方程是求固定策略的情况下，在指定状态s下能得到的评价的期望；而贝尔曼最优方程，增加了一个max，所求的是在不固定策略的情况下，在指定状态s下，所能得到的评价的期望的最大值。即方程中有两个未知量，在某一个policy下，会有最大的v

# Maximization on the right side of BOE
要求解上述公式，看起来非常复杂，有pai和v两个未知数。可以先考虑以下例子
![[5b3801c8-1335-49c9-9904-ac5d91b9b96f.png]]
同样有两个未知数；先求右边的最大值，将x视为常数，易有当y=0时有最大值；此时将y=0带入，去掉max，再求解x=2x-1即可

![[b62b2b9e-6dfb-4204-92d9-7f39a8e8c76b.png]]
所以同样可以对贝尔曼最优公式，先求解右边的最大值。可以视为q(s,a)是已知的（其中有转移概率与奖励概率，即环境为已知；而v(s')会给一个初值），于是要做的就是把pai(a|s)确定下来。那么只看右边的最大值该怎么求呢？考虑以下这个例子
![[c77b6a0d-76cc-4de7-99eb-46e72988b197.png]]
其中q相当于q(s,a)，而c1+c2+c3=1，满足概率的和为一。假如q1>q2>q3，那么最大值仅当c1=1，c2=c3=0时取得。从而可以知道选取q(s,a)最大的a的概率为1时有最大值
![[02d6d293-7ee4-4838-a6b9-b95da5c20f39.png]]

# Solve BOE
现在具体到如何去求解BOE右边的部分，这个带max的式子。用下面这个action value的式子来表示会更加直观。对于贝尔曼方程，其中pai是固定的，即在s选择每个a的概率是确定不变的，每一轮迭代求解中不断计算即可。
而对于贝尔曼最优方程的求解，虽然说有一个max，但是求这个max并不需要将所有可能的pai都列出来，然后选择最大的那一个。而是根据上面的推导，选择出对应q最大的pai即可。即这个pai是变化的，根据上面的推导，只需要将q最大的a所对应的概率设为1即可，每一次迭代，pai都可能变化。
![[b62b2b9e-6dfb-4204-92d9-7f39a8e8c76b.png]]


![[2c8fd0a6-cfb4-4db1-b2bb-ffdd6e52d8ac.png]]
现在令BOE右边为f(v)，从而有 v=f(v)

现在引入两个概念，不动点与收缩函数
![[572242f4-1bd3-4b98-b358-57939d6b1dce.png]]
![[f0d97bc3-2333-43a4-8274-586d8b877ee2.png]]
显而易见，不动点指经过函数变换后不变的点；而收缩函数（contractive function）是指经过函数变换后任意两点间距离变小

于是引入contraction mapping theorem
该定理对于x=f(x)，且fx是收缩函数的形式说明了三件事
1. 存在一个不动点
2. 该点唯一
3. 可以通过迭代方法计算
![[a436eb0b-9944-4c37-92fa-844b8e35cef1.png]]

而可以求得BOE的右边f(v)也是一个收缩函数，可以知道贝尔曼最优公式可以通过迭代方法求解出来。令对应的v为v*，pai为pai*
![[061cf73c-4991-4177-9a40-f609690122c9.png]]
于是有这样一个特殊的贝尔曼公式，即贝尔曼最优公式；解出来的结果对应pai* 的样子如下，只会选择最好的动作
![[a3bb7570-5cbd-478e-a1a9-365943333f0e.png]]

# Analyzing optimal polices
什么决定了最优的策略——BOE中的已知部分
![[1224f05d-09ce-4b8c-8e53-989906321022.png]]
即reward r；系统模型p；折扣率γ

那么当改变r或者γ时会发生什么（系统模型一般难改变）：当γ变小，模型会越来越短视，更重视近期的return，策略会改变；如果让r=ar+b，发现策略并不会改变，即说明策略考虑的是r之间的相对关系
