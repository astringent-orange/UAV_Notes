
正式介绍强化学习的model-based算法。先总结一下，前面两章贝尔曼方程与贝尔曼最优方程，是在描述策略的好坏与如何找到最好的策略；而本章是介绍如何去求解两个方程（虽然前面两章已经有了部分介绍）。注意，目的一直是去找到最好的策略。

# 1. Value iteration algorithm
在上一章的内容中，通过以下迭代方法求解了贝尔曼最优方程，该方法被称为 value iteration 值迭代
![[99369ef7-71fc-408d-9739-1dfd3d943bb6.png]]
具体而言，其分为两个步骤，首先给定一个值v0
1. policy update：对于给定的vk，求解出一个pai让右侧的内容最大
2. value update：将上面求得的最大值带入，求解vk+1


# 2. Policy iteration
首先给定一个policy pai0，有以下两步
1. policy evaluation （PE）：前面提到过，给定一个策略，求解其贝尔曼公式的过程就是PE；一种是闭式的，一种是迭代的
![[bacc972d-840a-4bc7-98b4-ca18b94d6d05.png]]
2. policy improvement：在求出pai k之后根据以下公式求出一个更好的pai k+1的过程；实际上就是上一步中求出了v之后，有q，让pai直接选取q最大的动作即可
![[79ba872b-94d2-4768-b801-1d92ac1cc4f7.png]]
即如何找到最好的策略呢？PI首先要求出当前策略下每个状态的优略，从而求出每个action的优略；然后根据q去更新策略，即让策略选择最好的action

那么两种算法之间有什么关系呢？


# 3. Truncated policy iteration algorithm
该算法是前面两个算法的一般推广，或者反过来说，VI与PI是TPI的极端情况

PI是从一个状态出发，而VI是从一个值出发；
两种算法是很相似的，可以用下图表示；可以看到VI是没有初始的策略的
![[fbbdcbf4-4b7d-4773-806a-c3b2e6aeee47.png]]
更具体的对比如下，这里为了对比，让VI的初值v0=v pai0（v0其实可以是任意值）；而第三步两者都是相同的，都有同一个pai。但是第四步两者发生了不同，PI是在求解贝尔曼方程得到的v pai1，v1是直接求得的；两者有什么不同？
![[87b797ba-e02a-48f6-95e4-0f7125cc5ce4.png]]
PI是符合直觉的方法，对于每一步（一个策略）都通过迭代法完全求出各action的价值，然后更新策略；而VI没有那么符合直觉，其本身就是求解贝尔曼最优方程的方法（联系上一章一起看），即本身是一个迭代过程（PI中求v的过程是迭代过程）

PI通过迭代法来解贝尔曼方程，迭代无穷多次得到v pai1，但是VI在第一次迭代就停下了，得到v1。于是想到是否有一个中间步，只迭代j次的方法？即truncated policy iteration
![[27bf1ade-6e70-4e3e-b06c-1d9f3777ac44.png]]
但其实PI的算法只在理论中存在，实际是不存在的，通常会判断两次迭代值之间的差距是否足够小来实现。而TPI不管差距，只管迭代的次数。
可以证明在以下情况下该算法是收敛的
![[a22739ab-95a4-48cf-ab5a-ba7290df9ef1.png]]