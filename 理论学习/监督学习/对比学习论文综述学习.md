
**什么是对比学习**
一种自监督学习方法（即在无标签数据集上的学习），能较好地学习到general feature。通过比较“数据对的”相似“或者”不同“来获取高阶信息。以simclrv2为例，要区分猫和狗。将一张猫的图片扩增为2张，则这两张图片之间为positive pair，而与狗的扩增图像之间为negative pair。将一对positive pair放入模型，分别生成两个向量表征，目标是让这两个向量表征之间相似度最大。

从19年开始，cv乃至ml最炽手可热的方向之一，盘活了17年开始就很卷的cv领域

*三步骤*

data augmentation 数据扩增，形成数据对

encoding 将图像放入模型，得到一个向量表示

loss minimization 让pp之间相似度最大，np之间相似对最小



如何对比向量相似性：可使用向量之间夹角的cos



------

### 百花齐放（18-19年中）

**InstDisc** 

提出了**个体判别**任务，为moco提供了基础。受有监督学习启发，提出将每一个图片（instance）看做一个类别。即将图片通过cnn编为一个向量，在空间中每一个向量尽可能分开，通过对比学习来实现，采用NCE loss，不同图片之间即为负样本。将所有向量存放于**memory bank**中。使用了proximal regularization 使mb中的向量动态更新

**InvaSpread**

SimCLR前身，仍然使用个体判别进行对比学习，没有使用额外数据结构存储负样本，正负样本来自同一个minibatch。相似的物体特征尽量相似，不同的特征则分开。关键在于如何选取正负样本。使用一个编码器进行端到端的学习。

**CPC**

不同于个体判别的判别式任务，cpc是**生成式**的代理任务。以RNN等**自回归**为例，某一时刻的上下文向量ct如果足够好，则可以预测未来，从而预测结果和真实结果之间是正样本。

**CMC**

同样也是预测式任务，定义正样本的方式更广泛。认为从不同的视角看同一物体，仍然具体相似性。**不同模态**之间的对同一物体的图片之间互为正样本。较早地进行了多视角的对比学习，为CLIP提供思路。意识到对比学习的灵活性。但有局限性在于，不同视角的数据需要多个编码器来处理。



------

### CV双雄（19年中-20年中）

**MoCov1**

将之前的对比学习方法归纳总结为一个**字典查询**的问题，提出了**队列**和**动量编码器**。在instdisc基础上，使用队列取代了memory bank，存储负样本；使用动量编码器取代loss的约束项，动量更新编码器。基线是resnet，基本是instdisc的改进。论文写的很好，自顶向下。

**SimCLRv1**

简单的对比学习方法。创新点在于得到向量表征之后，再加一个mlp层，映射到了更小的维度，大幅提升了效率。正负样本都来自同一mini-batch。使用了更多的数据增强方法。

**MoCov2**

在moco基础上结合simclr的技术，即添加mlp层和使用更多数据增强

**SimCLRv2**

论文主题simclarv2、微调、作为老师生成伪标签。从v1到v2：扩大模型，将mlp变成两层，也使用动量编码器。

**SWaV**

将对比学习和聚类方法结合。认为对比学习中将特征互相对比太过低效且原始，提出与聚类的中心比



------

### 不用负样本

**BYOL**



**SimSiam**



------

### 基于Transformer

**MoCov3**



**DINO**